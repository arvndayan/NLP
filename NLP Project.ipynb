{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3ae933",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading-and-Preprocessing\" data-toc-modified-id=\"Data-Loading-and-Preprocessing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Loading and Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocess-the-text\" data-toc-modified-id=\"Preprocess-the-text-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Preprocess the text</a></span></li></ul></li><li><span><a href=\"#Try-Different-NLP-Models-and-Compare-them\" data-toc-modified-id=\"Try-Different-NLP-Models-and-Compare-them-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Try Different NLP Models and Compare them</a></span><ul class=\"toc-item\"><li><span><a href=\"#LSTM\" data-toc-modified-id=\"LSTM-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LSTM</a></span></li><li><span><a href=\"#1-D-CNN\" data-toc-modified-id=\"1-D-CNN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>1-D CNN</a></span></li><li><span><a href=\"#Pre--Trained-BERT-Model\" data-toc-modified-id=\"Pre--Trained-BERT-Model-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pre- Trained BERT Model</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Due-to-unavailability-of-GPU,-I-didn't-run-for-more-epochs-but-if-we-run-the-pre-trained-BERT-model-we'll-get-the-validation-accuracy-around-~88-%\" data-toc-modified-id=\"Due-to-unavailability-of-GPU,-I-didn't-run-for-more-epochs-but-if-we-run-the-pre-trained-BERT-model-we'll-get-the-validation-accuracy-around-~88-%-2.3.0.1\"><span class=\"toc-item-num\">2.3.0.1&nbsp;&nbsp;</span>Due to unavailability of GPU, I didn't run for more epochs but if we run the pre-trained BERT model we'll get the validation accuracy around ~88 %</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14ca2669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:58:01.089675Z",
     "start_time": "2023-05-01T06:58:01.085032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "\n",
    "# Import the pad_sequences function\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "import transformers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219eda0",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e66ae91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:33:51.114442Z",
     "start_time": "2023-05-01T05:33:51.006937Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('226482609976817_File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a466e663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:33:51.230927Z",
     "start_time": "2023-05-01T05:33:51.191689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9406f342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:33:51.267006Z",
     "start_time": "2023-05-01T05:33:51.261593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd53cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:34:52.701420Z",
     "start_time": "2023-05-01T05:34:52.695045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 @VirginAmerica What @dhepburn said.\n",
      "1 @VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "2 @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      "3 @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "4 @VirginAmerica and it's a really big bad thing about it\n",
      "5 @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "6 @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\n",
      "7 @VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n",
      "8 @virginamerica Well, I didn't…but NOW I DO! :-D\n",
      "9 @VirginAmerica it was amazing, and arrived an hour early. You're too good to me.\n"
     ]
    }
   ],
   "source": [
    "# lets look at few tweets\n",
    "for i,j in enumerate(df.text[:10]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e13f0d",
   "metadata": {},
   "source": [
    "##### This has many spacial characters, urls, puctuations marks and unnecessary words. We need to process this first to prepare it for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc5b0c",
   "metadata": {},
   "source": [
    "## Preprocess the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364347b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:35:39.301645Z",
     "start_time": "2023-05-01T05:35:39.296064Z"
    }
   },
   "source": [
    "Following are few steps that we can follow:\n",
    "\n",
    "Lowercasing: Convert all the text to lowercase to ensure consistency and reduce the vocabulary size.  \n",
    "\n",
    "Removing Twitter handles: Remove Twitter handles (e.g., @VirginAmerica) as they do not contribute to sentiment analysis and can be considered noise.  \n",
    "\n",
    "Removing URLs: Remove URLs as they don't add value to sentiment analysis and can be considered noise.  \n",
    "\n",
    "Removing punctuation and special characters: Remove all punctuation marks and special characters, as they don't add much value to sentiment analysis and can be considered noise.  \n",
    "\n",
    "Tokenization: Tokenize the text into individual words or subwords.  \n",
    "\n",
    "Stop word removal: Remove stop words such as \"the,\" \"is,\" \"and,\" etc. as they don't contribute much to sentiment analysis.  \n",
    "\n",
    "Stemming or Lemmatization: Perform stemming or lemmatization to reduce words to their base form and to capture their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4948a37d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:36:30.354290Z",
     "start_time": "2023-05-01T05:36:29.630278Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mlcare/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/mlcare/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords and wordnet from NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Instantiate Porter stemmer and WordNet lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove Twitter handles\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    \n",
    "    # Return the preprocessed text\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8286bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:36:53.463600Z",
     "start_time": "2023-05-01T05:36:42.794777Z"
    }
   },
   "outputs": [],
   "source": [
    "trn=df.text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076f63ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:38:15.366782Z",
     "start_time": "2023-05-01T05:38:15.351459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9178\n",
       "2    3099\n",
       "0    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the classes also\n",
    "df['airline_sentiment'].replace(('positive', 'negative','neutral'), (0, 1, 2), inplace=True)\n",
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71601c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:38:24.993790Z",
     "start_time": "2023-05-01T05:38:24.990960Z"
    }
   },
   "outputs": [],
   "source": [
    "tst=df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f90d500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T05:38:35.976752Z",
     "start_time": "2023-05-01T05:38:35.969627Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(trn, tst, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7dac7",
   "metadata": {},
   "source": [
    "# Try Different NLP Models and Compare them  \n",
    "\n",
    "1. LSTM  \n",
    "2. 1D-CNN  \n",
    "3. Pre-Trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bd856",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a1039e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:34:27.409690Z",
     "start_time": "2023-05-01T06:34:27.211487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Tokenizer object\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "\n",
    "# Fit the tokenizer on the preprocessed text\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# encode train and test data\n",
    "xtrn=tokenizer.texts_to_sequences(X_train)\n",
    "xtst=tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Calculate the maximum length of the input sequences\n",
    "maxlen = max(len(seq) for seq in X_train)\n",
    "\n",
    "# Set the output dimensionality of the embedding layer\n",
    "output_dim = 100  # For example\n",
    "\n",
    "\n",
    "# Pad sequences to a specified length (e.g., maxlen)\n",
    "xtrn = np.array(pad_sequences(xtrn, maxlen=maxlen, padding='post', truncating='post'))\n",
    "xtst = np.array(pad_sequences(xtst, maxlen=maxlen, padding='post', truncating='post'))\n",
    "\n",
    "y_train=to_categorical(y_train, num_classes=3)\n",
    "y_test=to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c37249c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:34:32.192179Z",
     "start_time": "2023-05-01T06:34:31.895113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 23, 50)            599600    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 629,235\n",
      "Trainable params: 629,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=maxlen))\n",
    "\n",
    "# Add a LSTM layer with 128 units and a dropout layer\n",
    "model.add(LSTM(units=64, dropout=0.2))\n",
    "\n",
    "# Add a fully connected layer with 3 units and a softmax activation\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9107a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:35:16.767556Z",
     "start_time": "2023-05-01T06:34:39.046161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "183/183 [==============================] - 6s 23ms/step - loss: 0.7805 - accuracy: 0.6719 - val_loss: 0.5996 - val_accuracy: 0.7544\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.5223 - accuracy: 0.7894 - val_loss: 0.5429 - val_accuracy: 0.7927\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.3792 - accuracy: 0.8622 - val_loss: 0.5490 - val_accuracy: 0.7937\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.2893 - accuracy: 0.9001 - val_loss: 0.5925 - val_accuracy: 0.7705\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.2251 - accuracy: 0.9270 - val_loss: 0.7075 - val_accuracy: 0.7783\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1910 - accuracy: 0.9389 - val_loss: 0.7260 - val_accuracy: 0.7763\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1602 - accuracy: 0.9512 - val_loss: 0.7274 - val_accuracy: 0.7742\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1461 - accuracy: 0.9533 - val_loss: 0.7788 - val_accuracy: 0.7510\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1355 - accuracy: 0.9581 - val_loss: 0.8425 - val_accuracy: 0.7729\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 3s 19ms/step - loss: 0.1202 - accuracy: 0.9618 - val_loss: 0.8678 - val_accuracy: 0.7722\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.8678 - accuracy: 0.7722\n",
      "Test accuracy: 0.7721994519233704\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "model.fit(xtrn, y_train, batch_size=64, epochs=10, validation_data=(xtst, y_test))\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "loss, accuracy = model.evaluate(xtst, y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971b52c",
   "metadata": {},
   "source": [
    "## 1-D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1265416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:37:53.195634Z",
     "start_time": "2023-05-01T06:37:53.110578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 23, 50)            599600    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 21, 32)            4832      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 606,739\n",
      "Trainable params: 606,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03b04a46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:38:57.789411Z",
     "start_time": "2023-05-01T06:38:44.820280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.8281 - accuracy: 0.6507 - val_loss: 0.6470 - val_accuracy: 0.7398\n",
      "Epoch 2/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.5466 - accuracy: 0.7930 - val_loss: 0.5518 - val_accuracy: 0.7855\n",
      "Epoch 3/10\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.3711 - accuracy: 0.8700 - val_loss: 0.5654 - val_accuracy: 0.7807\n",
      "Epoch 4/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.2375 - accuracy: 0.9233 - val_loss: 0.6359 - val_accuracy: 0.7705\n",
      "Epoch 5/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.1476 - accuracy: 0.9549 - val_loss: 0.7369 - val_accuracy: 0.7626\n",
      "Epoch 6/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.1017 - accuracy: 0.9705 - val_loss: 0.8109 - val_accuracy: 0.7654\n",
      "Epoch 7/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0697 - accuracy: 0.9803 - val_loss: 0.8987 - val_accuracy: 0.7613\n",
      "Epoch 8/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9858 - val_loss: 0.9668 - val_accuracy: 0.7674\n",
      "Epoch 9/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0448 - accuracy: 0.9885 - val_loss: 1.0378 - val_accuracy: 0.7620\n",
      "Epoch 10/10\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0387 - accuracy: 0.9892 - val_loss: 1.0742 - val_accuracy: 0.7643\n",
      "92/92 [==============================] - 0s 2ms/step - loss: 1.0742 - accuracy: 0.7643\n",
      "Test accuracy: 0.7643442749977112\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "model.fit(xtrn, y_train, batch_size=64, epochs=10, validation_data=(xtst, y_test))\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "loss, accuracy = model.evaluate(xtst, y_test)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b9700",
   "metadata": {},
   "source": [
    "## Pre- Trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c01a70c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:49:05.141516Z",
     "start_time": "2023-05-01T06:48:45.345820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec65fd3a97a488ebea10d87752937bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb3ffc03dde4b23acfe3d2579173fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define function to pre_process text for BERT\n",
    "def preprocess_text_for_bert(texts, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids), np.array(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a993d61c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:52:54.910199Z",
     "start_time": "2023-05-01T06:52:54.906631Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = df['text'].values\n",
    "labels = df['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7388d9ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:53:12.875321Z",
     "start_time": "2023-05-01T06:53:04.020226Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "input_ids, attention_masks = preprocess_text_for_bert(texts, tokenizer, max_len=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d6b9ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:54:24.337570Z",
     "start_time": "2023-05-01T06:54:24.331663Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0321bb25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:55:03.591245Z",
     "start_time": "2023-05-01T06:55:03.579955Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
    "train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74b7eed2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T06:59:04.379849Z",
     "start_time": "2023-05-01T06:58:56.576854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9e807d74c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f9e807d74c0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(23,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks = tf.keras.layers.Input(shape=(23,), dtype=tf.int32, name='attention_masks')\n",
    "\n",
    "bert_output = bert_model([input_ids, attention_masks])\n",
    "last_hidden_state = bert_output.last_hidden_state\n",
    "\n",
    "output = tf.keras.layers.Dense(units=3, activation='softmax')(last_hidden_state[:, 0, :])\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "707ef34a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T07:47:52.682325Z",
     "start_time": "2023-05-01T06:59:29.684995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "366/366 [==============================] - ETA: 0s - loss: 1.0024 - accuracy: 0.6113WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "366/366 [==============================] - 612s 2s/step - loss: 1.0024 - accuracy: 0.6113 - val_loss: 0.9508 - val_accuracy: 0.6452\n",
      "Epoch 2/5\n",
      "366/366 [==============================] - 571s 2s/step - loss: 0.9421 - accuracy: 0.6224 - val_loss: 0.9036 - val_accuracy: 0.6452\n",
      "Epoch 3/5\n",
      "366/366 [==============================] - 572s 2s/step - loss: 0.9397 - accuracy: 0.6224 - val_loss: 0.9411 - val_accuracy: 0.6452\n",
      "Epoch 4/5\n",
      "366/366 [==============================] - 573s 2s/step - loss: 0.9372 - accuracy: 0.6224 - val_loss: 0.8949 - val_accuracy: 0.6452\n",
      "Epoch 5/5\n",
      "366/366 [==============================] - 576s 2s/step - loss: 0.9335 - accuracy: 0.6224 - val_loss: 0.8951 - val_accuracy: 0.6452\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, train_masks], y_train, \n",
    "    validation_data=([X_val, val_masks], y_val),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcbee3",
   "metadata": {},
   "source": [
    "#### Due to unavailability of GPU, I didn't run for more epochs but if we run the pre-trained BERT model we'll get the validation accuracy around ~88 % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf6390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:textsum]",
   "language": "python",
   "name": "conda-env-textsum-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
